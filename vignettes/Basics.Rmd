---
title: "Basic Usage of fashr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Basics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup}
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
library(fashr)
```

```{r}
# Plot power against FDR
power_versus_fdr <- function(fdr_result, true_indices, fdr_vec = seq(0.01,0.99,by = 0.01), plot = TRUE){
  power <- numeric(length(fdr_vec))
  for (i in 1:length(fdr_vec)){
    significant_indices <- fdr_result$index[fdr_result$FDR < fdr_vec[i]]
    power[i] <- length(intersect(significant_indices, true_indices))/length(true_indices)
  }
  result <- data.frame(fdr = fdr_vec, power = power)
  if (plot){
    plot(fdr_vec, power, type = "l", xlab = "FDR", ylab = "Power")
  }
  return(result)
}

# Plot calibration of FDR
calibration_fdr <- function(fdr_result, true_indices, fdr_vec = seq(0.01,0.99,by = 0.01), plot = TRUE){
  true_discovery_rate <- numeric(length(fdr_vec))
  for (i in 1:length(fdr_vec)){
    significant_indices <- fdr_result$index[fdr_result$FDR < fdr_vec[i]]
    true_discovery_rate[i] <- length(intersect(significant_indices, true_indices))/length(significant_indices)
  }
  result <- data.frame(fdr = fdr_vec, Tfdr = (1 - true_discovery_rate))
  if (plot){
    plot(Tfdr ~ fdr_vec, type = "l", xlab = "nominal FDR", ylab = "observed FDR", data = result)
    lines(c(0,1), c(0,1), col = "red", lty = 2)
  }
  return(result)
}
```

## Overview

Suppose we have $N$ sets of eQTLs. The effect size estimate for the $i$th eQTL at time $t_{ir}$, where $r \in \{1, \ldots, R_i\}$, is denoted by $\hat{\beta}_i(t_{ir})$.
Given the true effect $\beta_i(t_{ir})$, the observed estimate is assumed to follow
\[
\hat{\beta}_i(t_{ir}) \mid \beta_i(t_{ir}) \overset{ind}{\sim} \mathcal{N}(\beta_i(t_{ir}), s_{ir}^2),
\]
where $s_{ir}$ denotes the standard error at time $t_{ir}$.
Let $\boldsymbol{\hat{\beta}}_i = [\hat{\beta}_i(t_{i1}), \ldots, \hat{\beta}_i(t_{iR_i})] \in \mathbb{R}^{R_i}$ and $\boldsymbol{s}_i = [s_{i1}, \ldots, s_{iR_i}] \in \mathbb{R}^{R_i}$ denote the vectors of effect size estimates and standard errors for the $i$th eQTL, respectively. 

The goal of `fashr` is to perform functional adaptive shrinkage (FASH) for inferring the posterior distribution of the effect size function $\beta_i(t)$, given the observed data $\boldsymbol{\hat{\beta}}_i$ and $\boldsymbol{s}_i$.


FASH assumes that all $\beta_i(t)$ are i.i.d. draws from a common prior $g_\beta$. The prior $g_\beta$ has the form of a finite mixture of Gaussian processes (GPs):
\[
g_\beta \mid \boldsymbol{\pi} = \sum_{k=0}^{K} \pi_k \, \text{GP}_k.
\]
Each GP component $\text{GP}_k$ is specified via the differential equation: if $\beta(t) \sim \text{GP}_k$, then
\[
L\beta(t) = \sigma_k W(t),
\]
where $W(t)$ is a Gaussian white noise process and $L$ is a known $p$th-order linear differential operator. The standard deviation parameter $\sigma_k$ determines how much $\beta(t)$ can deviate from the base model, defined as the null space $S_0 = \text{Null}\{L\}$.

Given a grid of $\{\sigma_1, ..., \sigma_K\}$, FASH estimates the prior mixing weights $\hat{\boldsymbol{\pi}}$ by maximizing the marginal likelihood of the observed effect size estimates:
\[
\hat{\boldsymbol{\pi}} = (\hat{\pi}_1, ..., \hat{\pi}_K) = \arg\max_{\boldsymbol{\pi}} \sum_{i=1}^{N} \log\left(\sum_{k=0}^{K} \pi_k \mathbf{L}_{ik}\right),
\]
where $\mathbf{L}_{ik}$ denotes the marginal likelihood of $\boldsymbol{\hat{\beta}}_i$ under the $k$th GP component.
Based on the estimated prior $\hat{g}_\beta$, FASH then computes the posterior distribution for $\beta_i(t)$ as:
\[
p(\beta_i(t) \mid \boldsymbol{\hat{\beta}}_i, \hat{\boldsymbol{\pi}}) = \sum_{k=0}^{K} \tilde{\pi}_k \, p_k(\beta_i(t) \mid \boldsymbol{\hat{\beta}}_i),
\]
where $p_k(\beta_i(t) \mid \boldsymbol{\hat{\beta}}_i)$ is the posterior distribution under the $k$th GP component.

In the following section, we illustrate this with simulated data.



## Setup

We consider effect size estimates for $N = 100$ eQTLs measured from day $t = 1$ to day $t = 16$:

- There are 50 eQTLs that are non-dynamic, that is, their effect sizes remain constant over time (Category A).
- There are 30 eQTLs with linear dynamics, that is, their effect sizes change linearly over time (Category B).
- There are 20 eQTLs with nonlinear dynamics, that is, their effect sizes change nonlinearly over time (Category C).

For each eQTL at each time point, the standard error $s_{ir}$ is randomly drawn from $\{0.05, 0.1, 0.2\}$.


```{r}
set.seed(1)
N <- 100
propA <- 0.5; propB <- 0.3; propC <- 0.2
sigma_vec <- c(0.05, 0.1, 0.2)

sizeA <- N * propA
data_sim_list_A <- lapply(1:sizeA, function(i) simulate_process(sd_poly = 0.2, type = "nondynamic", sd = sigma_vec, normalize = TRUE))

sizeB <- N * propB
if(sizeB > 0){
data_sim_list_B <- lapply(1:sizeB, function(i) simulate_process(sd_poly = 1, type = "linear", sd = sigma_vec, normalize = TRUE))

}else{
  data_sim_list_B <- list()
}

sizeC <- N * propC
data_sim_list_C <- lapply(1:sizeC, function(i) simulate_process(sd_poly = 0, type = "nonlinear", sd = sigma_vec, sd_fun = 1, p = 1, normalize = TRUE))

datasets <- c(data_sim_list_A, data_sim_list_B, data_sim_list_C)
labels <- c(rep("A", sizeA), rep("B", sizeB), rep("C", sizeC))
indices_A <- 1:sizeA
indices_B <- (sizeA + 1):(sizeA + sizeB)
indices_C <- (sizeA + sizeB + 1):(sizeA + sizeB + sizeC)

dataset_labels <- rep(as.character(NA),100)
dataset_labels[indices_A] <- paste0("A",seq(1,length(indices_A)))
dataset_labels[indices_B] <- paste0("B",seq(1,length(indices_B)))
dataset_labels[indices_C] <- paste0("C",seq(1,length(indices_C)))
names(datasets) <- dataset_labels

par(mfrow = c(3, 3))
for(i in indices_A[1:3]){
  plot(datasets[[i]]$x, datasets[[i]]$y, type = "p", col = "black", lwd = 2, xlab = "Time", ylab = "Effect Size", ylim = c(-1.5, 1.5), main = paste("Category A: ", i))
  lines(datasets[[i]]$x, datasets[[i]]$truef, col = "red", lwd = 1)
}

for(i in indices_B[1:3]){
  plot(datasets[[i]]$x, datasets[[i]]$y, type = "p", col = "black", lwd = 2, xlab = "Time", ylab = "Effect Size", ylim = c(-1.5, 1.5), main = paste("Category B: ", i))
  lines(datasets[[i]]$x, datasets[[i]]$truef, col = "red", lwd = 1)
}

for(i in indices_C[1:3]){
  plot(datasets[[i]]$x, datasets[[i]]$y, type = "p", col = "black", lwd = 2, xlab = "Time", ylab = "Effect Size", ylim = c(-1.5, 1.5), main = paste("Category C: ", i))
  lines(datasets[[i]]$x, datasets[[i]]$truef, col = "red", lwd = 1)
}

par(mfrow = c(1, 1))
```

Let's take a look at the data structure:

```{r}
length(datasets)
str(datasets[[1]])
```

Take a look at the true label of the datasets:

```{r}
table(labels)
```

## Fitting FASH

The default way of fitting FASH is to input the list of datasets
(`data_list`), and specify the column names for the effect size (`Y`),
the standard deviation of the effect size (`S`), and the time points
(`smooth_var`).

The computation could be paralleled by specifying the number of cores
(`num_cores`). Reducing the number of basis functions (`num_basis`)
can also greatly speed up the computation.

### Testing Nonlinear Dynamic

We first aim to detect dynamic eQTLs with nonlinear dynamics (Category C).

In this case, we specify the base model $S_0 = {\beta(t) = c + bt \mid c, b \in \mathbb{R}}$ as the space of linear functions. Therefore, we choose $L = \frac{d^2}{dt^2}$, which is a second-order differential operator. This choice corresponds to the second Integrated Wiener Process (IWP2) model (`order = 2`).

We aim to obtain the posterior for each $\beta_i(t)$ and test the null hypothesis $H_0: \beta_i(t) \in S_0$ at a given FDR level.

```{r}
fash_fit <- fash(Y = "y", smooth_var = "x", S = "sd", data_list = datasets, 
                  num_cores = 2, num_basis = 20, grid = seq(0, 1, by = 0.025),
                  verbose = TRUE)
fash_fit
```

Here, the grid of values $\{\sigma_1, \ldots, \sigma_K\}$ is specified using the `grid` argument. Rather than being defined on the original scale, the `grid` is specified on a slightly transformed scale for easier interpretation.

Specifically, given a prediction step size $h$ (`pred_step`), each grid point is defined as $c_h \sigma_k$, where $c_h$ is a positive scaling constant that depends only on $h$ and $L$. This scaling ensures that $c_h \sigma_k$ can be interpreted as $\text{SD}(\beta(t+h) \mid \beta(s), s < t)$, representing the $h$-step predictive standard deviation (PSD).


Let's take a look at the estimated prior $\hat{g}$:

```{r}
fash_fit$prior_weights
```

We can take a look at their posterior weights in each GP component:

```{r}
plot(fash_fit, plot_type = "heatmap", selected_indices = c(1:5, 55:60, 95:100))
```

Besides the heatmap plot, we could also visualize the result using a structure plot:

```{r}
plot(fash_fit, plot_type = "structure", 
     discrete = TRUE, selected_indices = c(1:5, 55:60, 95:100))
```


We can then use `fdr_control` to test the null hypothesis that $H_0: \beta_i(t) \in S_0$ at a given FDR level:

```{r}
fdr_result <- fdr_control(fash_fit, alpha = 0.1, plot = TRUE)
```

There are `r sum(fdr_result$fdr_results$FDR < 0.1)` eQTLs flagged as significant at FDR level 0.1. We can take out the indices of these eQTLs:

```{r}
detected_indices <- fdr_result$fdr_results$index[fdr_result$fdr_results$FDR < 0.1]
```

How many of the true (non-linear) dynamic eQTLs are detected?

```{r}
sum(labels[detected_indices] == "C")/sizeC
```

What is the false discovery rate?

```{r}
sum(labels[detected_indices] != "C")/length(detected_indices)
```

Let's take a look at the inferred eQTL effect $\beta_i(t)$ for the detected eQTLs. 

```{r}
fitted_beta <- predict(fash_fit, index = detected_indices[1])
str(fitted_beta)
```

The `predict` function returns the posterior information of the effect size $\beta_i(t)$ for the eQTL specified by `index`.
By default, it returns the posterior of the effect size at each observed time point.
We can also specify the time points to predict the effect size at:

```{r}
fitted_beta_new <- predict(fash_fit, index = detected_indices[1], smooth_var = seq(0, 16, length.out = 100))
str(fitted_beta_new)
```

It is also possible to store M posterior samples rather than the posterior summary:

```{r}
fitted_beta_samples <- predict(fash_fit, index = detected_indices[1], 
                               smooth_var = seq(0, 16, length.out = 100), 
                               only.samples = TRUE, M = 50)
str(fitted_beta_samples)
```

Let's plot the inferred effect size for the first detected eQTL:

```{r}
plot(datasets[[detected_indices[1]]]$x, datasets[[detected_indices[1]]]$y, type = "p", col = "black", lwd = 2, xlab = "Time", ylab = "Effect Size")
lines(fitted_beta_new$x, fitted_beta_new$mean, col = "red", lwd = 2)
lines(datasets[[detected_indices[1]]]$x, datasets[[detected_indices[1]]]$truef, col = "black", lwd = 1, lty = 2)
polygon(c(fitted_beta_new$x, rev(fitted_beta_new$x)), c(fitted_beta_new$lower, rev(fitted_beta_new$upper)), col = rgb(1, 0, 0, 0.2), border = NA)
```

### Testing Dynamic eQTLs

We may also be interested in detecting any dynamic eQTLs, including both linear and nonlinear cases (Categories B and C).

In this case, the base model $S_0 = \{\beta(t) = c \mid c \in \mathbb{R}\}$ represents the space of constant functions. We specify $L = \frac{d}{dt}$, which is a first-order differential operator. 
This choice corresponds to the first Integrated Wiener Process (IWP1) model (`order = 1`).

```{r}
fash_fit_2 <- fash(Y = "y", smooth_var = "x", S = "sd", data_list = datasets, 
                  likelihood = "gaussian", order = 1, pred_step = 1,
                  num_cores = 2, num_basis = 20, grid = seq(0, 1, by = 0.025),
                  verbose = TRUE)
fash_fit_2
```

Take a look at the structure plot:

```{r}
plot(fash_fit_2, discrete = TRUE, ordering = "mean")
```

Let's test the null hypothesis that $H_0: \beta_i(t) \in S_0$ at a
given FDR level (specifying `alpha = 0.1`):

```{r}
fdr_result_2 <- fdr_control(fash_fit_2, alpha = 0.1, plot = TRUE)
detected_indices_2 <- fdr_result_2$fdr_results$index[fdr_result_2$fdr_results$FDR < 0.1]
```

How many of the true dynamic eQTLs are detected?

```{r}
sum(labels[detected_indices_2] != "A")/(sizeB + sizeC)
```

What is the false discovery rate?

```{r}
sum(labels[detected_indices_2] == "A")/length(detected_indices_2)
```


