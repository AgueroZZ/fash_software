likelihood = settings$likelihood
)
# Compute probability of being positive/negative
prob_df <- compute_posterior_prob(marginal_stats$mean, marginal_stats$var)
# Compute LFSR and return minimum value
lfsr <- pmin(prob_df$pos_prob, prob_df$neg_prob)
return(min(lfsr))
}
lfsr_df2[1:5,]
lfsr_df[1:5,]
lfsr_df3[1:5,]
lfsr_df4 <- min_lfsr_summary(fash_fit = fash_fit, index = 32)
lfsr_df4 <- min_lfsr_summary(object = fash_fit, index = 32)
compute_posterior_sign_prob <- function(mu, sigma2) {
# Validate inputs
if (length(mu) != length(sigma2)) stop("mu and sigma2 must have the same length.")
if (any(sigma2 < 0)) stop("sigma2 must be non-negative.")
# Convert variance to standard deviation
sigma <- sqrt(sigma2)
# Compute probabilities using normal CDF
pos_prob <- numeric(length(mu))
neg_prob <- numeric(length(mu))
# Case when sigma = 0 (point mass)
zero_sigma_idx <- sigma == 0
nonzero_sigma_idx <- !zero_sigma_idx
if (any(nonzero_sigma_idx)) {
pos_prob[nonzero_sigma_idx] <- 1 - pnorm(-mu[nonzero_sigma_idx] / sigma[nonzero_sigma_idx])
neg_prob[nonzero_sigma_idx] <- pnorm(-mu[nonzero_sigma_idx] / sigma[nonzero_sigma_idx])
}
if (any(zero_sigma_idx)) {
pos_prob[zero_sigma_idx] <- ifelse(mu[zero_sigma_idx] > 0, 1, ifelse(mu[zero_sigma_idx] < 0, 0, 0.5))
neg_prob[zero_sigma_idx] <- ifelse(mu[zero_sigma_idx] > 0, 0, ifelse(mu[zero_sigma_idx] < 0, 1, 0.5))
}
# Return data frame
return(data.frame(pos_prob = pos_prob, neg_prob = neg_prob))
}
compute_marginal_mean_var_once <- function(data_i, refined_x, psd_iwp, Si = NULL, Omegai = NULL, num_basis = 30, betaprec = 1e-6, order = 2, pred_step = 1, likelihood) {
# Create the tmbdat object using the existing helper function
tmbdat <- fash_set_tmbdat(data_i, Si, Omegai, num_basis = num_basis, betaprec = betaprec, order = order)
# Extract smoothing variables and response
y <- data_i$y
x <- data_i$x
offset <- data_i$offset
# Generate spline knots for the smoothing variable
knots <- seq(min(x), max(x), length.out = num_basis)
# Compute the refined matrix
design_all <- BayesGP:::global_poly_helper(refined_x, p = order)
if (psd_iwp != 0) {
B_refined <- BayesGP:::local_poly_helper(knots = knots, refined_x = refined_x, p = order)
design_all <- cbind(B_refined, design_all)
} else{
return(data.frame(mean = rep(0, length(refined_x)), var = rep(0, length(refined_x))))
}
# Determine the DLL for TMB based on likelihood and error structure
DLL <- switch(likelihood,
"gaussian" = if (!is.null(Si)) {
if (psd_iwp != 0) "Gaussian_ind" else "Gaussian_ind_fixed"
} else {
if (psd_iwp != 0) "Gaussian_dep" else "Gaussian_dep_fixed"
},
"poisson" = if (psd_iwp != 0) "Poisson_ind" else "Poisson_ind_fixed",
stop("Unknown likelihood function. Choose 'gaussian' or 'poisson'."))
# Add sigmaIWP for random effects
if (psd_iwp != 0) {
tmbdat$sigmaIWP <- psd_iwp / sqrt((pred_step ^ ((2 * order) - 1)) / (((2 * order) - 1) * (factorial(order - 1) ^ 2)))
}
# Define initial parameter values
tmbparams <- list(
W = rep(0, if (psd_iwp != 0) ncol(tmbdat$X) + ncol(tmbdat$B) else ncol(tmbdat$X))
)
# Create the TMB model
ff <- TMB::MakeADFun(
data = tmbdat,
parameters = tmbparams,
DLL = DLL,
silent = TRUE
)
# Define Hessian for optimization
ff$he <- function(w) numDeriv::jacobian(ff$gr, w)
# Optimize the model
opt <- stats::nlminb(
start = ff$par,
objective = ff$fn,
gradient = ff$gr,
hessian = ff$he,
control = list(eval.max = 20000, iter.max = 20000)
)
# Extract precision matrix from Hessian
prec_matrix <- Matrix::forceSymmetric(ff$he(opt$par))
var_matrix <- B_refined %*% solve(prec_matrix)[(1:nrow(B_refined)), (1:ncol(B_refined))] %*% t(B_refined)
var <- diag(var_matrix)
mean <- B_refined %*% opt$par[1:ncol(B_refined)]
data.frame(mean = mean, var = var)
}
compute_marginal_mean_var <- function(data_i, posterior_weights, psd_values, refined_x,
Si = NULL, Omegai = NULL, num_basis = 30, betaprec = 1e-6,
order = 2, pred_step = 1, likelihood) {
# Validate inputs
if (length(posterior_weights) != length(psd_values)) {
stop("posterior_weights and psd_values must have the same length.")
}
# Normalize posterior weights to sum to 1
posterior_weights <- posterior_weights / sum(posterior_weights)
# Initialize matrices to store results
mean_matrix <- matrix(0, nrow = length(refined_x), ncol = length(psd_values))
var_matrix <- matrix(0, nrow = length(refined_x), ncol = length(psd_values))
# Loop through each PSD value and compute mean and variance
for (j in seq_along(psd_values)) {
psd_iwp <- psd_values[j]
mean_var_df <- compute_marginal_mean_var_once(
data_i, refined_x, psd_iwp,
Si = Si, Omegai = Omegai, num_basis = num_basis, betaprec = betaprec,
order = order, pred_step = pred_step, likelihood = likelihood
)
mean_matrix[, j] <- mean_var_df$mean
var_matrix[, j] <- mean_var_df$var
}
# Compute weighted mean and variance
weighted_mean <- mean_matrix %*% posterior_weights
weighted_var <- (var_matrix + mean_matrix^2) %*% posterior_weights - weighted_mean^2
# Return final data frame
return(data.frame(mean = weighted_mean, var = weighted_var))
}
#' Compute Minimum Local False Sign Rate (LFSR) from Marginal Posterior Mean and Variance
#'
#' This function computes the minimum LFSR for a specific dataset in a `fash` object using
#' the posterior mean and variance instead of sampling-based methods.
#'
#' @param object A `fash` object containing the fitted results.
#' @param index An integer specifying the dataset index. Default is `1`.
#' @param smooth_var A numeric vector specifying refined x values for evaluation.
#'   If `NULL`, defaults to the dataset's original x values.
#'
#' @return A numeric value representing the minimum LFSR for the selected dataset.
#'
#' @examples
#' # Example usage
#' data_list <- list(
#'   data.frame(y = rpois(5, lambda = 5), x = 1:5, offset = 0),
#'   data.frame(y = rpois(5, lambda = 5), x = 1:5, offset = 0)
#' )
#' grid <- seq(0, 2, length.out = 10)
#' fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = TRUE)
#'
#' # Compute min LFSR for dataset 1
#' min_lfsr <- min_lfsr_summary(fash_obj, index = 1)
#' print(min_lfsr)
#'
#' @export
min_lfsr_summary <- function(object, index = 1, smooth_var = NULL) {
# Validate input
if (!inherits(object, "fash")) {
stop("Input must be a `fash` object.")
}
if (index < 1 || index > length(object$posterior_weights)) {
stop("Index is out of range for the datasets in the `fash` object.")
}
# Extract dataset-specific components
data_i <- object$fash_data$data_list[[index]]
Si <- object$fash_data$S[[index]]
Omegai <- object$fash_data$Omega[[index]]
psd_values <- object$prior_weights$psd
posterior_weights <- object$posterior_weights[index, ]
# Use smooth_var if provided; otherwise, default to the dataset's x values
refined_x <- if (!is.null(smooth_var)) smooth_var else data_i$x
# Retrieve settings from fash object
settings <- object$settings
# Compute marginal mean and variance using BMA weighting
marginal_stats <- compute_marginal_mean_var(
data_i = data_i,
posterior_weights = posterior_weights,
psd_values = psd_values,
refined_x = refined_x,
Si = Si,
Omegai = Omegai,
num_basis = settings$num_basis,
betaprec = settings$betaprec,
order = settings$order,
pred_step = settings$pred_step,
likelihood = settings$likelihood
)
# Compute probability of being positive/negative
prob_df <- compute_posterior_prob(marginal_stats$mean, marginal_stats$var)
# Compute LFSR and return minimum value
lfsr <- pmin(prob_df$pos_prob, prob_df$neg_prob)
return(min(lfsr))
}
lfsr_df4 <- min_lfsr_summary(object = fash_fit, index = 32)
source("~/Desktop/fash_software/R/05_lfsr.R")
lfsr_df4 <- min_lfsr_summary(object = fash_fit, index = 32)
source("~/Desktop/fash_software/R/05_lfsr.R")
lfsr_df4 <- min_lfsr_summary(object = fash_fit, index = 32)
lfsr_df4
lfsr_df2[1:5,]
lfsr_df1[1:5,]
lfsr_df[1:5,]
lfsr_df4 <- min_lfsr_summary(object = fash_fit, index = 200)
lfsr_df4
lfsr_df[52,]
lfsr_df[52:60,]
min_lfsr_summary(object = fash_fit, index = 108)
min_lfsr_summary(object = fash_fit, index = 108, smooth_var = t_vec)
min_lfsr_summary(object = fash_fit, index = 108, smooth_var = c(0,1,2,4))
min_lfsr_summary(object = fash_fit, index = 158, smooth_var = c(0,1,2,4))
min_lfsr_summary(object = fash_fit, index = 158, smooth_var = t_vec)
t_vec
t_vec <- seq(0, 5, length.out = 100)
min_lfsr_summary(object = fash_fit, index = 158, smooth_var = t_vec)
source("~/Desktop/fash_software/R/05_lfsr.R")
# Load required libraries
library(microbenchmark)
# Generate synthetic data for testing
set.seed(123)
n_datasets <- 20  # Adjust for larger testing
n_points <- 10
# Create example datasets
data_list <- lapply(1:n_datasets, function(i) {
data.frame(y = rpois(n_points, lambda = 5), x = seq(1, n_points), offset = 0)
})
# Define PSD grid values
grid <- seq(0, 2, length.out = 10)
# Fit the fash object
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE)
fash_obj
# Load required libraries
library(microbenchmark)
# Generate synthetic data for testing
set.seed(123)
n_datasets <- 20  # Adjust for larger testing
n_points <- 10
# Create example datasets
data_list <- lapply(1:n_datasets, function(i) {
df <- data.frame(x = seq(1, n_points), offset = 0)
df$y <- rpois(n_points, lambda = 1 + 0.5 * df$x)
})
# Define PSD grid values
grid <- seq(0, 2, length.out = 10)
# Fit the fash object
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE)
# Load required libraries
library(microbenchmark)
# Generate synthetic data for testing
set.seed(123)
n_datasets <- 20  # Adjust for larger testing
n_points <- 10
# Create example datasets
data_list <- lapply(1:n_datasets, function(i) {
df <- data.frame(x = seq(1, n_points), offset = 0)
df$y <- rpois(n_points, lambda = 1 + 0.5 * df$x)
df
})
# Define PSD grid values
grid <- seq(0, 2, length.out = 10)
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE)
fash_obj
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE, order = 1)
fash_obj
# Load required libraries
library(microbenchmark)
# Generate synthetic data for testing
set.seed(123)
n_datasets <- 20  # Adjust for larger testing
n_points <- 10
# Create example datasets
data_list <- lapply(1:n_datasets, function(i) {
df <- data.frame(x = seq(from = 0, to = 1, length.out = n_points), offset = 0)
df$y <- rpois(n_points, lambda = exp(1 + 0.5 * df$x))
df
})
# Define PSD grid values
grid <- seq(0, 2, length.out = 10)
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE, order = 1)
fash_obj
# Define PSD grid values
grid <- seq(0, 1, length.out = 10)
# Fit the fash object
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE, order = 1)
fash_obj
fash_obj$prior_weights
data_list <- lapply(1:n_datasets, function(i) {
df <- data.frame(x = seq(from = 0, to = 1, length.out = n_points), offset = 0)
df$y <- rpois(n_points, lambda = exp(1 + rnorm(1) * df$x))
df
})
# Define PSD grid values
grid <- seq(0, 1, length.out = 10)
# Fit the fash object
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE, order = 1)
fash_obj
fash_obj$prior_weights
data_list <- lapply(1:n_datasets, function(i) {
df <- data.frame(x = seq(from = 0, to = 1, length.out = n_points), offset = 0)
df$y <- rpois(n_points, lambda = exp(rnorm(1, sd = 0.1) * df$x))
df
})
# Define PSD grid values
grid <- seq(0, 1, length.out = 10)
# Fit the fash object
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE, order = 1)
fash_obj
fash_obj$prior_weights
# Load required libraries
library(microbenchmark)
# Generate synthetic data for testing
set.seed(123)
n_datasets <- 100  # Adjust for larger testing
n_points <- 10
# Create example datasets
data_list <- lapply(1:n_datasets, function(i) {
df <- data.frame(x = seq(from = 0, to = 1, length.out = n_points), offset = 0)
df$y <- rpois(n_points, lambda = exp(rnorm(1, sd = 0.1) * df$x))
df
})
# Define PSD grid values
grid <- seq(0, 1, length.out = 10)
# Fit the fash object
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE, order = 1)
fash_obj
fash_obj$prior_weights
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE, order = 1, penalty = 2)
fash_obj
fash_obj$prior_weights
data_list[[1]]
# Load required libraries
library(microbenchmark)
# Generate synthetic data for testing
set.seed(123)
n_datasets <- 500  # Adjust for larger testing
n_points <- 30
# Create example datasets
data_list <- lapply(1:n_datasets, function(i) {
df <- data.frame(x = seq(from = 0, to = 1, length.out = n_points), offset = 0)
df$y <- rpois(n_points, lambda = exp(rnorm(1, sd = 0.1) * df$x))
df
})
# Define PSD grid values
grid <- seq(0, 1, length.out = 10)
# Fit the fash object
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = FALSE, order = 1, penalty = 2)
fash_obj
fash_obj$prior_weights
set.seed(123)
n_datasets <- 100  # Adjust for larger testing
n_points <- 30
# Create example datasets
data_list <- lapply(1:n_datasets, function(i) {
df <- data.frame(x = seq(from = 0, to = 3, length.out = n_points), offset = 0)
df$y <- rpois(n_points, lambda = exp(rnorm(1, sd = 0.1) * df$x))
df
})
# Define PSD grid values
grid <- seq(0, 1, length.out = 10)
# Fit the fash object
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = TRUE, order = 1, penalty = 1)
fash_obj
fash_obj$psd_grid
fash_obj$prior_weights
# Define PSD grid values
grid <- seq(0, 0.1, length.out = 10)
# Fit the fash object
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = TRUE, order = 1, penalty = 1)
fash_obj
fash_obj$prior_weights
# Create example datasets
data_list <- lapply(1:n_datasets, function(i) {
df <- data.frame(x = seq(from = 0, to = 3, length.out = n_points), offset = 0)
df$y <- rpois(n_points, lambda = exp(3 + rnorm(1, sd = 0.1) * df$x))
df
})
# Define PSD grid values
grid <- seq(0, 0.1, length.out = 10)
# Fit the fash object
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = TRUE, order = 1, penalty = 1)
fash_obj
fash_obj$prior_weights
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = TRUE, order = 1, penalty = 2)
fash_obj
fash_obj$prior_weights
# Set the number of cores for parallel execution
num_cores <- 1  # Adjust based on your machine's capabilities
# Benchmarking min_lfsr_sampling (sampling-based method)
runtime_sampling <- microbenchmark(
min_lfsr_sampling(fash_obj, num_cores = num_cores, M = 3000),
times = 5  # Number of repetitions for averaging runtime
)
runtime_sampling
# Benchmarking min_lfsr_summary (analytical method)
runtime_summary <- microbenchmark(
min_lfsr_summary(fash_obj, num_cores = num_cores),
times = 5  # Number of repetitions for averaging runtime
)
source("~/Desktop/fash_software/R/05_lfsr.R")
# Benchmarking min_lfsr_summary (analytical method)
runtime_summary <- microbenchmark(
min_lfsr_summary(fash_obj, num_cores = num_cores),
times = 5  # Number of repetitions for averaging runtime
)
runtime_summary
runtime_sampling
# Print runtime comparisons
print(runtime_sampling)
print(runtime_summary)
# Compare execution times visually
boxplot(runtime_sampling$time / 1e9, runtime_summary$time / 1e9,
names = c("Sampling", "Summary"),
ylab = "Time (seconds)", main = "Runtime Comparison: Sampling vs Summary")
num_cores
fashr:::fash_eb_est
fashr::fash
# Chunk 1
collapse_L <- function(L, log = F) {
if (ncol(L) > 1) {
pi_hat_star <- mixsqp::mixsqp(L = L[, -1, drop = FALSE],
log = log,
control = list(verbose = FALSE))$x
} else{
pi_hat_star <- rep(1, nrows(L))
}
L_c <- matrix(0, nrow = nrow(L), ncol = 2)
L_c[, 1] <- L[, 1]
L_c[, 2] <- (L[, -1, drop = FALSE] %*% pi_hat_star)
return(L_c)
}
# Chunk 2
BF_control <- function(BF, plot = T) {
BF_sorted <- sort(BF, decreasing = F)
mu <- numeric(length(BF_sorted))
pi0_hat <- numeric(length(BF_sorted))
mu <- cumsum(BF_sorted) / seq_along(BF_sorted)
pi0_hat <- seq_along(BF_sorted) / length(BF_sorted)
if (max(mu) < 1) {
pi0_hat_star <- 1
return(list(mu = mu, pi0_hat = pi0_hat, pi0_hat_star = pi0_hat_star))
}else{
pi0_hat_star <- pi0_hat[mu >= 1][1]
}
if (plot) {
par(mfrow = c(1, 2))
hist(log(BF_sorted), breaks = 100, freq = T, xlab = "log-BF", main = "Histogram of log-BF")
abline(v = log(BF_sorted)[mu >= 1][1], col = "red")
plot(pi0_hat, mu, type = "l", xlab = "est pi0", ylab = "E(BF | BF <= c)", xlim=c(0,1), ylim = c(0,3))
abline(h = 1, col = "red")
par(mfrow = c(1, 1))
}
return(list(mu = mu, pi0_hat = pi0_hat, pi0_hat_star = pi0_hat_star))
}
## simulate n observation from mixture of two normal distributions
n <- 30000
# Specify the true alternative hypothesis of N(0, true_sd)
true_sd <- 1
pi0_true <- 0.1
x <- c(rep(0, ceiling(n*pi0_true)), rnorm(n = ceiling(n*(1-pi0_true)), mean = 0, sd = true_sd))
## simulate y based on x
sebetahat <- 1
y <- x + rnorm(n, mean = 0, sd = sebetahat)
hist(y, breaks = 50)
## fit ash with a fitted alternative hypothesis of N(0, fit_sd)
fit_sd <- c(0.1,0.3,0.5)
ash_fit <- ashr::ash(betahat = y, sebetahat = sebetahat, mixcompdist = "normal", mixsd = fit_sd, nullweight = 1, outputlevel = 3)
ash_fit$fitted_g$pi[1]
L_mat <- ash_fit$fit_details$matrix_lik
## fit biased MoM
mean(apply(ash_fit$fit_details$matrix_lik, 1, which.max) == 1)
data_list <- list(
data.frame(y = rpois(5, lambda = 5), x = 1:5, offset = 0),
data.frame(y = rpois(5, lambda = 5), x = 1:5, offset = 0)
)
grid <- seq(0, 2, length.out = 10)
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = TRUE)
library(fashr)
data_list <- list(
data.frame(y = rpois(5, lambda = 5), x = 1:5, offset = 0),
data.frame(y = rpois(5, lambda = 5), x = 1:5, offset = 0)
)
grid <- seq(0, 2, length.out = 10)
fash_obj <- fash(data_list = data_list, Y = "y", smooth_var = "x", grid = grid, likelihood = "poisson", verbose = TRUE)
fash_obj$psd_grid
1/0
0/0
log(0)
source("~/Desktop/fash_software/R/06_BF.R")
load("~/Desktop/FASH/FASHresultsummary/output/dynamic_eQTL_real/fash_fit1_all.RData")
L_mat <- exp(fash_fit1$L_matrix)
L_c <- collapse_L(L_mat)
BF <- L_c[, 2] / L_c[, 1]
collapse_L
dim(L_c)
collapse_result <- L_c
L_c <- collapse_result$L_c
BF <- L_c[, 2] / L_c[, 1]
pi0_hat_star <- BF_control(BF)
pi0_hat_star
BF_update
source("~/Desktop/fash_software/R/06_BF.R")
BF_update
source("~/Desktop/fash_software/R/06_BF.R")
BF_update
fash_fit1_update <- BF_update(fash_fit1)
fash_fit1_update$prior_weight
fash_fit1_update$prior_weights
sum(fash_fit1_update$prior_weight$prior_weight)
sum(fash_fit1_update$prior_weights$prior_weight)
fash_fit1_update$lfdr
fash_fit1_update$posterior_weights[,1]
fash_fit1_update$posterior_weights[1:5,1]
fash_fit1_update$posterior_weight[1:5,1]
fash_fit1_update$lfdr[1:5,1]
fash_fit1_update$lfdr[1:5]
source("~/Desktop/fash_software/R/06_BF.R")
fash_fit1_update <- BF_update(fash_fit1)
fash_fit1_update$prior_weights
fash_fit1_update$BF
